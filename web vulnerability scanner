import requests
from bs4 import BeautifulSoup
import urllib.parse


BASE_URL = 'https://httpbin.org'  


def fetch_page(url):
    try:
        print(f"Fetching URL: {url}")  
        response = requests.get(url)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

def find_links(page_content, base_url):
    soup = BeautifulSoup(page_content, 'html.parser')
    links = set()
    for a_tag in soup.find_all('a', href=True):
        link = urllib.parse.urljoin(base_url, a_tag['href'])
        links.add(link)
    return links


def check_vulnerabilities(url):

    common_paths = ['admin/', 'login/', 'uploads/', 'files/']
    for path in common_paths:
        test_url = urllib.parse.urljoin(url, path)
        print(f"Checking {test_url}...")
        response = requests.head(test_url)
        if response.status_code == 200:
            print(f"Potential vulnerability found: {test_url}")

def main():
    print(f"Starting scan on {BASE_URL}")

    
    content = fetch_page(BASE_URL)
    if not content:
        print("Failed to fetch base URL.")
        return

    
    links = find_links(content, BASE_URL)
    print(f"Found links: {links}")

    
    for link in links:
        print(f"Scanning {link}...")
        page_content = fetch_page(link)
        if page_content:
            check_vulnerabilities(link)

    print("Scan completed.")

if __name__ == "__main__":
    main()
